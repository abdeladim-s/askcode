{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AskCode API Reference","text":""},{"location":"#askcode.main","title":"askcode.main","text":"<p>This file contains the definition of AskCode main class</p>"},{"location":"#askcode.main.AskCode","title":"AskCode","text":"<pre><code>AskCode(\n    codebase_path,\n    language,\n    parser_threshold,\n    text_splitter_chunk_size,\n    text_splitter_chunk_overlap,\n    use_HF,\n    llm_model,\n    embeddings_model,\n    retriever_search_type,\n    retriever_k,\n    max_new_tokens,\n    temperature,\n    top_p,\n    repetition_penalty,\n    use_autogptq,\n)\n</code></pre> Source code in <code>askcode/main.py</code> <pre><code>def __init__(self,\n             codebase_path: str,\n             language: str,\n             parser_threshold: int,\n             text_splitter_chunk_size: int,\n             text_splitter_chunk_overlap: int,\n             use_HF: bool,\n             llm_model: str,\n             embeddings_model: str,\n             retriever_search_type: str,\n             retriever_k: int,\n             max_new_tokens: int,\n             temperature: float,\n             top_p: float,\n             repetition_penalty: float,\n             use_autogptq: bool,\n             ):\n    self.codebase_path = Path(codebase_path)\n    self.language = language\n    self.parser_threshold = parser_threshold\n    self.text_splitter_chunk_size = text_splitter_chunk_size\n    self.text_splitter_chunk_overlap = text_splitter_chunk_overlap\n\n    self.use_HF = use_HF\n    self.llm_model = llm_model\n    self.embeddings_model = embeddings_model\n\n    self.retriever_search_type = retriever_search_type\n    self.retriever_k = retriever_k\n\n    self.use_autogptq = use_autogptq\n\n    self.max_new_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.repetition_penalty = repetition_penalty\n</code></pre>"},{"location":"#askcode.main.AskCode.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Sets up the Necessary components for the langchain chain</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>askcode/main.py</code> <pre><code>def setup(self) -&gt; None:\n\"\"\"\n     Sets up the Necessary components for the langchain chain\n\n    :return: None\n    \"\"\"\n    with console.status(\"[bold green]Loading files ...\") as status:\n        self.retriever = self.load_retriever()\n        console.log(f\"[bold green]Files loaded successfully\")\n\n    with console.status(\"[bold green]Loading LLM ...\") as status:\n        self.llm = self.load_llm()\n        console.log(f\"[bold green]LLM loaded successfully\")\n\n    self.prompt_template = self.get_prompt_template()\n</code></pre>"},{"location":"#askcode.main.AskCode.load_retriever","title":"load_retriever","text":"<pre><code>load_retriever()\n</code></pre> <p>Loads the files from the codebase and sets up the retriever</p> Source code in <code>askcode/main.py</code> <pre><code>def load_retriever(self):\n\"\"\"\n    Loads the files from the codebase and sets up the retriever\n    \"\"\"\n    try:\n        with open(self.codebase_path / '.gitignore') as f:\n            exclude = f.readlines()\n    except Exception as e:\n        # no gitignore found\n        exclude = []\n\n    loader = GenericLoader.from_filesystem(\n        self.codebase_path,\n        glob=\"**/[!.]*\",\n        exclude=exclude,\n        suffixes=[\".py\", \".js\"],  # only python and javascript atm\n        show_progress=True,\n        parser=LanguageParser(language=self.language, parser_threshold=self.parser_threshold)\n    )\n    files = loader.load()\n    splitter = RecursiveCharacterTextSplitter.from_language(language=self.language,\n                                                   chunk_size=self.text_splitter_chunk_size,\n                                                   chunk_overlap=self.text_splitter_chunk_overlap)\n    docs = splitter.split_documents(files)\n\n    if self.use_HF:\n        db = Chroma.from_documents(docs, HuggingFaceEmbeddings(model_name=self.embeddings_model))\n    else:\n        # defaults to OpenAI\n        from langchain.embeddings import OpenAIEmbeddings\n        db = Chroma.from_documents(docs, OpenAIEmbeddings(disallowed_special=()))\n\n    retriever = db.as_retriever(\n        search_type=self.retriever_search_type,\n        search_kwargs={\"k\": self.retriever_k},\n    )\n\n    return retriever\n</code></pre>"},{"location":"#askcode.main.AskCode.load_llm","title":"load_llm","text":"<pre><code>load_llm()\n</code></pre> <p>Sets up the LLM</p> Source code in <code>askcode/main.py</code> <pre><code>def load_llm(self):\n\"\"\"\n    Sets up the LLM\n    \"\"\"\n    if self.use_HF:\n        tokenizer = AutoTokenizer.from_pretrained(self.llm_model, use_fast=True)\n        if self.use_autogptq:\n            from auto_gptq import AutoGPTQForCausalLM\n            model = AutoGPTQForCausalLM.from_quantized(self.llm_model, use_safetensors=True)\n        else:\n            model = AutoModelForCausalLM.from_pretrained(self.llm_model)\n\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            max_new_tokens=self.max_new_tokens,\n            temperature=self.temperature,\n            top_p=self.top_p,\n            repetition_penalty=self.repetition_penalty,\n        )\n        llm = HuggingFacePipeline(pipeline=pipe)\n        return llm\n    else:\n        # defaults to OpenAI\n        from langchain.chat_models import ChatOpenAI\n        return ChatOpenAI(model_name=self.llm_model)\n</code></pre>"},{"location":"#askcode.main.AskCode.get_prompt_template","title":"get_prompt_template","text":"<pre><code>get_prompt_template()\n</code></pre> <p>Sets up the prompt template</p> Source code in <code>askcode/main.py</code> <pre><code>def get_prompt_template(self):\n\"\"\"\n    Sets up the prompt template\n    \"\"\"\n    template = \"\"\"Use the following pieces of context to answer the question at the end. \n       If you don't know the answer, just say that you don't know, don't try to make up an answer. \n       Use three sentences maximum and keep the answer as concise as possible. \n{context}\n       Question: {question}\n       Helpful Answer:\"\"\"\n\n    prompt_template = PromptTemplate(\n        input_variables=[\"context\", \"question\"],\n        template=template,\n    )\n\n    return prompt_template\n</code></pre>"},{"location":"#askcode.main.AskCode.chain","title":"chain","text":"<pre><code>chain(retriever, llm, prompt_template, question)\n</code></pre> <p>Runs a question through Langchain Chain</p> <p>Parameters:</p> Name Type Description Default <code>retriever</code> <p>the docs Retriever</p> required <code>llm</code> <p>the large language model</p> required <code>prompt_template</code> <p>the prompt template</p> required <code>question</code> <code>str</code> <p>the question</p> required <p>Returns:</p> Type Description <p>chain results</p> Source code in <code>askcode/main.py</code> <pre><code>def chain(self, retriever, llm, prompt_template, question: str):\n\"\"\"\n    Runs a question through Langchain Chain\n\n    :param retriever: the docs Retriever\n    :param llm: the large language model\n    :param prompt_template: the prompt template\n    :param question: the question\n\n    :return: chain results\n    \"\"\"\n    relevant_docs = retriever.get_relevant_documents(question)\n    chain = load_qa_chain(llm, prompt=prompt_template)\n    return chain({\"input_documents\": relevant_docs, \"question\": question}, return_only_outputs=False)\n</code></pre>"},{"location":"#askcode.main.AskCode.ask","title":"ask","text":"<pre><code>ask(question)\n</code></pre> <p>Ask a question to the codebase You need to call <code>self.setup</code> before calling this function</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>the question :)</p> required <p>Returns:</p> Type Description <p>chain results</p> Source code in <code>askcode/main.py</code> <pre><code>def ask(self, question: str):\n\"\"\"\n    Ask a question to the codebase\n    You need to call `self.setup` before calling this function\n\n    :param question: the question :)\n    :return: chain results\n    \"\"\"\n    return self.chain(self.retriever, self.llm, self.prompt_template, question)\n</code></pre>"},{"location":"#askcode.cli","title":"askcode.cli","text":"<p>Command Line Interface</p>"},{"location":"#askcode.cli.main","title":"main","text":"<pre><code>main(\n    codebase_path=\".\",\n    language=\"python\",\n    parser_threshold=0,\n    text_splitter_chunk_size=256,\n    text_splitter_chunk_overlap=50,\n    use_HF=True,\n    llm_model=\"TheBloke/CodeLlama-7B-GPTQ\",\n    embeddings_model=\"sentence-transformers/all-MiniLM-L12-v2\",\n    retriever_search_type=\"mmr\",\n    retriever_k=4,\n    max_new_tokens=50,\n    temperature=0.1,\n    top_p=0.9,\n    repetition_penalty=1.0,\n    use_autogptq=True,\n)\n</code></pre> <p>Chat with your code base with the power of LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>codebase_path</code> <code>str</code> <p>path to your codebase</p> <code>'.'</code> <code>language</code> <code>str</code> <p>programming language ['python', 'javascript'] at the moment</p> <code>'python'</code> <code>parser_threshold</code> <code>int</code> <p>minimum lines needed to activate parsing (0 by default).</p> <code>0</code> <code>text_splitter_chunk_size</code> <code>int</code> <p>Maximum size of chunks to return</p> <code>256</code> <code>text_splitter_chunk_overlap</code> <code>int</code> <p>Overlap in characters between chunks</p> <code>50</code> <code>use_HF</code> <code>bool</code> <p>use hugging face models, if False OpenAI models will be used</p> <code>True</code> <code>llm_model</code> <code>str</code> <p>Large language model name (HF model name or OpenAI model)</p> <code>'TheBloke/CodeLlama-7B-GPTQ'</code> <code>embeddings_model</code> <code>str</code> <p>Embeddings model (HF model name or OpenAI model)</p> <code>'sentence-transformers/all-MiniLM-L12-v2'</code> <code>retriever_search_type</code> <code>str</code> <p>Defines the type of search that the Retriever should perform. Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\".</p> <code>'mmr'</code> <code>retriever_k</code> <code>int</code> <p>Amount of documents to return (Default: 4)</p> <code>4</code> <code>max_new_tokens</code> <code>int</code> <p>Maximum tokens to generate</p> <code>50</code> <code>temperature</code> <code>float</code> <p>sampling temperature</p> <code>0.1</code> <code>top_p</code> <code>float</code> <p>sampling top_p</p> <code>0.9</code> <code>repetition_penalty</code> <code>float</code> <p>sampling repetition_penalty</p> <code>1.0</code> <code>use_autogptq</code> <code>bool</code> <p>Set it to True to use Quantized AutoGPTQ models</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>askcode/cli.py</code> <pre><code>def main(\n    codebase_path: str = '.',\n    language: str = 'python',\n    parser_threshold: int = 0,\n    text_splitter_chunk_size: int = 256,\n    text_splitter_chunk_overlap: int = 50,\n    use_HF: bool = True,\n    llm_model: str = \"TheBloke/CodeLlama-7B-GPTQ\",\n    embeddings_model: str = \"sentence-transformers/all-MiniLM-L12-v2\",\n    retriever_search_type: str = \"mmr\",\n    retriever_k: int = 4,\n    max_new_tokens: int = 50,\n    temperature: float = 0.1,\n    top_p: float = 0.9,\n    repetition_penalty: float = 1.,\n    use_autogptq: bool = True):\n\"\"\"\n    Chat with your code base with the power of LLMs.\n\n    :param codebase_path: path to your codebase\n    :param language: programming language ['python', 'javascript'] at the moment\n    :param parser_threshold: minimum lines needed to activate parsing (0 by default).\n    :param text_splitter_chunk_size: Maximum size of chunks to return\n    :param text_splitter_chunk_overlap: Overlap in characters between chunks\n    :param use_HF: use hugging face models, if False OpenAI models will be used\n    :param llm_model: Large language model name (HF model name or OpenAI model)\n    :param embeddings_model: Embeddings model (HF model name or OpenAI model)\n    :param retriever_search_type: Defines the type of search that\n                the Retriever should perform.\n                Can be \"similarity\" (default), \"mmr\", or\n                \"similarity_score_threshold\".\n    :param retriever_k: Amount of documents to return (Default: 4)\n    :param max_new_tokens: Maximum tokens to generate\n    :param temperature: sampling temperature\n    :param top_p: sampling top_p\n    :param repetition_penalty: sampling repetition_penalty\n    :param use_autogptq: Set it to True to use Quantized AutoGPTQ models\n\n    :return: None\n    \"\"\"\n\n    ask_code = AskCode(codebase_path,\n                 language,\n                 parser_threshold,\n                 text_splitter_chunk_size,\n                 text_splitter_chunk_overlap,\n                 use_HF,\n                 llm_model,\n                 embeddings_model,\n                 retriever_search_type,\n                 retriever_k,\n                 max_new_tokens,\n                 temperature,\n                 top_p,\n                 repetition_penalty,\n                 use_autogptq)\n    console.print(__header__, style=\"blue\")\n    ask_code.setup()\n    console.print(\"CTRL+C To stop ...\", style=\"bold red\")\n    print()\n    while True:\n        try:\n            q = console.input(\"[yellow][-] How can I help you: \")\n            with console.status(\"[bold green]Searching ...\") as status:\n                res = ask_code.ask(q)\n                ans = enforce_stop_tokens(res['output_text'], [\"Question\"])\n                # ans = res['output_text']\n                console.print(f\"[+] Answer: {ans}\", style=\"bold green\")\n        except KeyboardInterrupt:\n            break\n</code></pre>"}]}